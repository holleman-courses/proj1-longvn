{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae35b60c-e8b9-4e85-8f2a-9b7881aed1b6",
   "metadata": {},
   "source": [
    "This code is modified from the visual wake word / person detection MLPerf Tiny Benchmark:\n",
    "https://github.com/mlcommons/tiny/tree/master/benchmark/training/visual_wake_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4780e-abb6-4f4d-aa1e-72af3cbb82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "from absl import app\n",
    "\n",
    "assert tf.__version__.startswith('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d4c8a-7907-4ee7-aae8-4a8ce5a138eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 96\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "validation_split = 0.1\n",
    "\n",
    "epochs = 20,10,20\n",
    "lrates = .001, .0005, .00025\n",
    "\n",
    "color_mode = 'grayscale'\n",
    "if color_mode == 'grayscale':\n",
    "  n_color_chans = 1\n",
    "elif color_mode == 'rgb':\n",
    "  n_color_chans = 3\n",
    "else:\n",
    "  raise ValueError(\"color_mode should be either 'rgb' or 'grayscale'\")\n",
    "\n",
    "# BASE_DIR = os.path.join(os.getcwd(), 'vw_coco2014_96')\n",
    "BASE_DIR = '/Users/jeremy/dev/tiny_mlperf/tiny_main/benchmark/training/visual_wake_words/vw_coco2014_96'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39306076-d1a6-4670-bdb6-c99fc46ae913",
   "metadata": {},
   "source": [
    "Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc56cf7-8c59-440b-a70e-a10bb315a0c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### vww_model.py\n",
    "'''\n",
    "MobilnetV1 from Silican Labs github page:\n",
    "https://github.com/SiliconLabs/platform_ml_models/blob/master/eembc/Person_detection/mobilenet_v1_eembc.py\n",
    "'''\n",
    "\n",
    "\n",
    "#define model\n",
    "def mobilenet_v1():\n",
    "    # Mobilenet parameters\n",
    "    input_shape = [96,96,1] # resized to 96x96 per EEMBC requirement # was [96,96,3]\n",
    "    num_classes = 2 # person and non-person\n",
    "    num_filters = 8 # normally 32, but running with alpha=.25 per EEMBC requirement\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs # Keras model uses ZeroPadding2D()\n",
    "\n",
    "    # 1st layer, pure conv\n",
    "    # Keras 2.2 model has padding='valid' and disables bias\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x) # Keras uses ReLU6 instead of pure ReLU\n",
    "\n",
    "    # 2nd layer, depthwise separable conv\n",
    "    # Filter size is always doubled before the pointwise conv\n",
    "    # Keras uses ZeroPadding2D() and padding='valid'\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 3rd layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 4th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 5th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 6th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 7th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 8th-12th layers, identical depthwise separable convs\n",
    "    # 8th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 9th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 10th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 11th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 12th\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 13th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=2,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    num_filters = 2*num_filters\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # 14th layer, depthwise separable conv\n",
    "    x = DepthwiseConv2D(kernel_size=3,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(num_filters,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Average pooling, max polling may be used also\n",
    "    # Keras employs GlobalAveragePooling2D \n",
    "    x = AveragePooling2D(pool_size=x.shape[1:3])(x)\n",
    "    #x = MaxPooling2D(pool_size=x.shape[1:3])(x)\n",
    "\n",
    "    # Keras inserts Dropout() and a pointwise Conv2D() here\n",
    "    # We are staying with the paper base structure\n",
    "\n",
    "    # Flatten, FC layer and classify\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e5407-1e65-4f51-adfb-27c14a430e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train_vww.py\n",
    "\"\"\"Training for the visual wakewords person detection model.\n",
    "\n",
    "The visual wakewords person detection model is a core model for the TinyMLPerf\n",
    "benchmark suite. This script provides source for how the reference model was\n",
    "created and trained, and can be used as a starting point for open submissions\n",
    "using re-training.\n",
    "\"\"\"\n",
    "\n",
    "model = mobilenet_v1()\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=validation_split,\n",
    "    rescale=1. / 255)\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training',\n",
    "    color_mode='grayscale') # was 'rgb'\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale') # was 'rgb'\n",
    "print(train_generator.class_indices)\n",
    "\n",
    "\n",
    "def train_epochs(model, train_generator, val_generator, epoch_count,\n",
    "                 learning_rate, steps_per_epoch=None):\n",
    "  model.compile(\n",
    "      optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "  if steps_per_epoch is None:\n",
    "    steps_per_epoch = len(train_generator)\n",
    "  history_fine = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=steps_per_epoch,\n",
    "      epochs=epoch_count,\n",
    "      validation_data=val_generator,\n",
    "      validation_steps=len(val_generator),\n",
    "      batch_size=BATCH_SIZE)\n",
    "  return model\n",
    "\n",
    "model = train_epochs(model, train_generator, val_generator, epochs[0], lrates[0])# steps_per_epoch=10)\n",
    "model = train_epochs(model, train_generator, val_generator, epochs[1], lrates[1])\n",
    "model = train_epochs(model, train_generator, val_generator, epochs[2], lrates[2])\n",
    "\n",
    "model.save('vww_96_tmp_res.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce0c338-70f9-4ec2-b757-00c536fbc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(val_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5defee-77cd-442f-88a3-667ef1cab28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e2112-b2b8-4c10-9128-430241ae54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From convert_vww.py\n",
    "\n",
    "# model = tf.keras.models.load_model(argv[1])\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "# with tf.io.gfile.GFile('vww_96_float.tflite', 'wb') as float_file:\n",
    "#   float_file.write(tflite_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset_gen():\n",
    "  dataset_dir = os.path.join(BASE_DIR, \"person\")\n",
    "  for idx, image_file in enumerate(os.listdir(dataset_dir)):\n",
    "    # 10 representative images should be enough for calibration.\n",
    "    if idx > 10:\n",
    "        return\n",
    "    full_path = os.path.join(dataset_dir, image_file)\n",
    "    if os.path.isfile(full_path):\n",
    "      img = tf.keras.preprocessing.image.load_img(\n",
    "          full_path, color_mode=color_mode).resize((96, 96))\n",
    "      arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "      # Scale input to [0, 1.0] like in training.\n",
    "      yield [arr.reshape(1, 96, 96, n_color_chans) / 255.] \n",
    "\n",
    "# Convert model to full-int8 and save as quantized tflite flatbuffer.\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "quantized_tflite_model = converter.convert()\n",
    "with tf.io.gfile.GFile('vww_96_int8.tflite', 'wb') as quantized_file:\n",
    "    quantized_file.write(quantized_tflite_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638696d-debc-44da-a616-a107743c76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild the val-set generator with batch-size = 1\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    BASE_DIR,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=1,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fab9e-7ff8-4bf6-82ac-974c50db7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = val_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c39249-1e5e-4a8d-8ef0-cde2fec59fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe36437-3c69-4057-bbb4-20eabf275b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(-b[0][0,:,:,0], cmap='Greys')\n",
    "print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f11d37-8b98-490c-a678-db2a9ef029e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(interpreter, datagen):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  labels = labels.squeeze()\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(inputs):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == labels).mean()\n",
    "  return accuracy, prediction_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64fa9d-a799-4696-a3a1-20a49413ad72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
